{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ðŸˆ MusangKing BALANCED Training v3\n",
                "## Fixing Class Imbalance Issue\n",
                "\n",
                "**Problem:** Model always predicts Musang King (73% of training data)\n",
                "\n",
                "**Solution:**\n",
                "- âš–ï¸ Undersample Musang King to balance classes\n",
                "- ðŸŽ¯ Use class weights in training\n",
                "- ðŸ“Š Better feature normalization"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install roboflow opencv-python-headless scikit-learn imbalanced-learn xgboost tqdm -q\n",
                "print(\"âœ… Dependencies installed!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load the features CSV from previous training (saves time!)\n",
                "# Upload your features.csv from trained_models_v2/TRAINING_MODEL/\n",
                "from google.colab import files\n",
                "print(\"ðŸ“¤ Upload features.csv from trained_models_v2/TRAINING_MODEL/\")\n",
                "uploaded = files.upload()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "from sklearn.model_selection import train_test_split, cross_val_score\n",
                "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
                "from sklearn.ensemble import RandomForestClassifier, VotingClassifier, GradientBoostingClassifier\n",
                "from sklearn.svm import SVC\n",
                "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
                "from imblearn.over_sampling import SMOTE\n",
                "from imblearn.under_sampling import RandomUnderSampler\n",
                "from imblearn.combine import SMOTETomek\n",
                "import xgboost as xgb\n",
                "import joblib\n",
                "import os\n",
                "\n",
                "# Load features\n",
                "df = pd.read_csv('features.csv')\n",
                "print(f\"Loaded {len(df)} samples\")\n",
                "print(\"\\nOriginal Distribution:\")\n",
                "print(df['Variety'].value_counts())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 1: BALANCE the Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# UNDERSAMPLE Musang King to match the average of other classes\n",
                "min_samples = df['Variety'].value_counts().min()  # 858 (Black Thorn)\n",
                "target_samples = int(min_samples * 1.5)  # ~1287 samples each\n",
                "\n",
                "print(f\"Target samples per class: ~{target_samples}\")\n",
                "\n",
                "# Balance by undersampling majority class\n",
                "balanced_dfs = []\n",
                "for variety in df['Variety'].unique():\n",
                "    variety_df = df[df['Variety'] == variety]\n",
                "    if len(variety_df) > target_samples:\n",
                "        # Undersample\n",
                "        variety_df = variety_df.sample(n=target_samples, random_state=42)\n",
                "    balanced_dfs.append(variety_df)\n",
                "\n",
                "df_balanced = pd.concat(balanced_dfs, ignore_index=True)\n",
                "print(\"\\nâœ… Balanced Distribution:\")\n",
                "print(df_balanced['Variety'].value_counts())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 2: Analyze Feature Differences"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check if features can actually distinguish varieties\n",
                "print(\"=== Feature Means by Variety ===\")\n",
                "feature_cols = ['Compactness', 'Smoothness', 'Aspect_Ratio', 'Rectangularity', 'Mean_Red', 'Mean_Hue']\n",
                "print(df_balanced.groupby('Variety')[feature_cols].mean())\n",
                "\n",
                "print(\"\\n=== Feature Standard Deviations ===\")\n",
                "print(df_balanced.groupby('Variety')[feature_cols].std())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 3: Train with BALANCED Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=\"*60)\n",
                "print(\"TRAINING VARIETY MODEL (BALANCED)\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "VARIETY_FEATURES = ['Compactness', 'Smoothness', 'Aspect_Ratio', 'Rectangularity', 'Mean_Red']\n",
                "\n",
                "X = df_balanced[VARIETY_FEATURES]\n",
                "y = df_balanced['Variety']\n",
                "\n",
                "variety_encoder = LabelEncoder()\n",
                "y_encoded = variety_encoder.fit_transform(y)\n",
                "print(f\"\\nClasses: {variety_encoder.classes_}\")\n",
                "\n",
                "# Feature Scaling (IMPORTANT!)\n",
                "scaler = StandardScaler()\n",
                "X_scaled = scaler.fit_transform(X)\n",
                "\n",
                "# Split\n",
                "X_train, X_test, y_train, y_test = train_test_split(\n",
                "    X_scaled, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
                ")\n",
                "\n",
                "print(f\"Train: {len(X_train)}, Test: {len(X_test)}\")\n",
                "print(f\"Train distribution: {pd.Series(y_train).value_counts().to_dict()}\")\n",
                "\n",
                "# Light SMOTE to even out any remaining imbalance\n",
                "smote = SMOTE(random_state=42, k_neighbors=3)\n",
                "X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)\n",
                "print(f\"After SMOTE: {pd.Series(y_train_balanced).value_counts().to_dict()}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Calculate class weights\n",
                "from sklearn.utils.class_weight import compute_class_weight\n",
                "class_weights = compute_class_weight('balanced', classes=np.unique(y_train_balanced), y=y_train_balanced)\n",
                "class_weight_dict = dict(zip(np.unique(y_train_balanced), class_weights))\n",
                "print(f\"Class weights: {class_weight_dict}\")\n",
                "\n",
                "# Build ensemble with class weights\n",
                "print(\"\\nðŸ”§ Training Balanced Ensemble...\")\n",
                "\n",
                "svm = SVC(\n",
                "    probability=True,\n",
                "    kernel='rbf',\n",
                "    C=10.0,\n",
                "    gamma='scale',\n",
                "    class_weight='balanced',\n",
                "    random_state=42\n",
                ")\n",
                "\n",
                "rf = RandomForestClassifier(\n",
                "    n_estimators=200,\n",
                "    max_depth=10,\n",
                "    min_samples_split=5,\n",
                "    class_weight='balanced',\n",
                "    random_state=42\n",
                ")\n",
                "\n",
                "xgb_clf = xgb.XGBClassifier(\n",
                "    n_estimators=200,\n",
                "    max_depth=5,\n",
                "    learning_rate=0.1,\n",
                "    scale_pos_weight=1,\n",
                "    random_state=42,\n",
                "    eval_metric='mlogloss'\n",
                ")\n",
                "\n",
                "gb = GradientBoostingClassifier(\n",
                "    n_estimators=100,\n",
                "    max_depth=4,\n",
                "    learning_rate=0.1,\n",
                "    random_state=42\n",
                ")\n",
                "\n",
                "# Equal voting weights\n",
                "variety_model = VotingClassifier(\n",
                "    estimators=[\n",
                "        ('svm', svm),\n",
                "        ('rf', rf),\n",
                "        ('xgb', xgb_clf),\n",
                "        ('gb', gb)\n",
                "    ],\n",
                "    voting='soft',\n",
                "    weights=[1, 1, 1, 1]  # Equal weights\n",
                ")\n",
                "\n",
                "variety_model.fit(X_train_balanced, y_train_balanced)\n",
                "print(\"âœ… Training complete!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Evaluate\n",
                "y_pred = variety_model.predict(X_test)\n",
                "y_pred_proba = variety_model.predict_proba(X_test)\n",
                "\n",
                "print(f\"\\nðŸŽ¯ Test Accuracy: {accuracy_score(y_test, y_pred)*100:.2f}%\")\n",
                "print(\"\\nClassification Report:\")\n",
                "print(classification_report(y_test, y_pred, target_names=variety_encoder.classes_))\n",
                "\n",
                "print(\"\\nConfusion Matrix:\")\n",
                "print(confusion_matrix(y_test, y_pred))\n",
                "\n",
                "# Check predictions per class\n",
                "print(\"\\nðŸ“Š Prediction Distribution on Test Set:\")\n",
                "pred_names = variety_encoder.inverse_transform(y_pred)\n",
                "print(pd.Series(pred_names).value_counts())\n",
                "\n",
                "# Confidence analysis\n",
                "confidences = np.max(y_pred_proba, axis=1)\n",
                "print(f\"\\nðŸ“ˆ Confidence: Mean={confidences.mean()*100:.1f}%, Min={confidences.min()*100:.1f}%, Max={confidences.max()*100:.1f}%\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 4: Train Ripeness Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=\"*60)\n",
                "print(\"TRAINING RIPENESS MODEL\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "RIPENESS_FEATURES = ['Mean_Hue', 'Compactness', 'Smoothness']\n",
                "\n",
                "# Use Mean_Saturation if available, else create synthetic labels\n",
                "if 'Mean_Saturation' in df_balanced.columns:\n",
                "    df_balanced['Ripeness'] = df_balanced.apply(\n",
                "        lambda row: 'defective' if row['Mean_Saturation'] < 50 or row['Compactness'] > 80 else 'mature', axis=1\n",
                "    )\n",
                "else:\n",
                "    # Simple heuristic based on available features\n",
                "    df_balanced['Ripeness'] = df_balanced.apply(\n",
                "        lambda row: 'defective' if row['Compactness'] > 150 else 'mature', axis=1\n",
                "    )\n",
                "\n",
                "X_ripe = df_balanced[RIPENESS_FEATURES]\n",
                "y_ripe = df_balanced['Ripeness']\n",
                "\n",
                "ripeness_encoder = LabelEncoder()\n",
                "y_ripe_encoded = ripeness_encoder.fit_transform(y_ripe)\n",
                "print(f\"Classes: {ripeness_encoder.classes_}\")\n",
                "print(f\"Distribution: {pd.Series(y_ripe_encoded).value_counts().to_dict()}\")\n",
                "\n",
                "X_train_r, X_test_r, y_train_r, y_test_r = train_test_split(\n",
                "    X_ripe, y_ripe_encoded, test_size=0.2, random_state=42, stratify=y_ripe_encoded\n",
                ")\n",
                "\n",
                "smote_r = SMOTE(random_state=42, k_neighbors=3)\n",
                "X_train_r_smote, y_train_r_smote = smote_r.fit_resample(X_train_r, y_train_r)\n",
                "\n",
                "ripeness_model = VotingClassifier(\n",
                "    estimators=[\n",
                "        ('svm', SVC(probability=True, class_weight='balanced', random_state=42)),\n",
                "        ('rf', RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42)),\n",
                "        ('xgb', xgb.XGBClassifier(n_estimators=100, random_state=42, eval_metric='mlogloss'))\n",
                "    ],\n",
                "    voting='soft'\n",
                ")\n",
                "\n",
                "ripeness_model.fit(X_train_r_smote, y_train_r_smote)\n",
                "\n",
                "y_pred_r = ripeness_model.predict(X_test_r)\n",
                "print(f\"\\nðŸŽ¯ Ripeness Accuracy: {accuracy_score(y_test_r, y_pred_r)*100:.2f}%\")\n",
                "print(classification_report(y_test_r, y_pred_r, target_names=ripeness_encoder.classes_))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 5: Save & Download"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "OUTPUT = \"TRAINING_MODEL_BALANCED\"\n",
                "os.makedirs(OUTPUT, exist_ok=True)\n",
                "\n",
                "joblib.dump(variety_model, f\"{OUTPUT}/variety_model.pkl\")\n",
                "joblib.dump(variety_encoder, f\"{OUTPUT}/variety_model_encoder.pkl\")\n",
                "joblib.dump(ripeness_model, f\"{OUTPUT}/ripeness_model.pkl\")\n",
                "joblib.dump(ripeness_encoder, f\"{OUTPUT}/ripeness_model_encoder.pkl\")\n",
                "\n",
                "print(\"âœ… Models saved!\")\n",
                "!ls -la {OUTPUT}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!zip -r balanced_models_v3.zip TRAINING_MODEL_BALANCED/\n",
                "\n",
                "from google.colab import files\n",
                "files.download('balanced_models_v3.zip')\n",
                "\n",
                "print(\"\\nðŸŽ‰ BALANCED TRAINING COMPLETE!\")\n",
                "print(\"1. Extract ZIP\")\n",
                "print(\"2. Copy .pkl files to 'TRAINING MODEL' folder\")\n",
                "print(\"3. Restart Flask\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}