{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üçà MusangKing ENHANCED Training\n",
                "## High-Confidence Classification for 3 Durian Varieties\n",
                "\n",
                "**Focus:** Musang King (D197), Black Thorn (D200), Udang Merah (D175)\n",
                "\n",
                "**Improvements:**\n",
                "- üéØ Data Augmentation (more training samples)\n",
                "- üìä Feature Scaling & Normalization\n",
                "- üîÑ Cross-Validation for robust accuracy\n",
                "- ü§ñ Optimized Ensemble Model\n",
                "- ‚öñÔ∏è Better Class Balancing (SMOTE + Class Weights)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 1: Install Dependencies"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install roboflow opencv-python-headless scikit-learn imbalanced-learn xgboost tqdm albumentations -q\n",
                "print(\"‚úÖ Dependencies installed!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 2: Download Datasets"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from roboflow import Roboflow\n",
                "\n",
                "API_KEY = \"EPa1uqQkD6BSITYjgrP3\"\n",
                "rf = Roboflow(api_key=API_KEY)\n",
                "\n",
                "print(\"üì• Downloading Dataset 1...\")\n",
                "project1 = rf.workspace(\"durian-cf87w\").project(\"durian_own\")\n",
                "dataset1 = project1.version(1).download(\"yolov5pytorch\")\n",
                "print(\"‚úÖ Dataset 1 downloaded!\\n\")\n",
                "\n",
                "print(\"üì• Downloading Dataset 2...\")\n",
                "project2 = rf.workspace(\"carl-bwzge\").project(\"durian-thesis\")\n",
                "dataset2 = project2.version(3).download(\"yolov5pytorch\")\n",
                "print(\"‚úÖ Dataset 2 downloaded!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 3: Import Libraries"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import cv2\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import os\n",
                "import yaml\n",
                "from tqdm import tqdm\n",
                "import joblib\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
                "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
                "from sklearn.ensemble import RandomForestClassifier, VotingClassifier, GradientBoostingClassifier\n",
                "from sklearn.svm import SVC\n",
                "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
                "from imblearn.over_sampling import SMOTE\n",
                "from imblearn.pipeline import Pipeline as ImbPipeline\n",
                "import xgboost as xgb\n",
                "\n",
                "print(\"‚úÖ Libraries imported!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 4: K-Means Segmentation (SAME AS APP.PY)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def get_mask_lab_method(image):\n",
                "    \"\"\"K-Means Segmentation - IDENTICAL TO APP.PY (Gamma=0.6, K=3)\"\"\"\n",
                "    img_resized = cv2.resize(image, (512, 512))\n",
                "    gamma = 0.6\n",
                "    table = np.array([((i / 255.0) ** (1.0/gamma)) * 255 for i in range(256)]).astype(\"uint8\")\n",
                "    img_gamma = cv2.LUT(img_resized, table)\n",
                "    img_lab = cv2.cvtColor(img_gamma, cv2.COLOR_BGR2Lab)\n",
                "    Z = img_lab.reshape((-1, 3)).astype(np.float32)\n",
                "    criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 10, 1.0)\n",
                "    _, label, center = cv2.kmeans(Z, 3, None, criteria, 10, cv2.KMEANS_RANDOM_CENTERS)\n",
                "    center = np.uint8(center)\n",
                "    result_image = center[label.flatten()].reshape((img_lab.shape))\n",
                "    h, w = result_image.shape[:2]\n",
                "    center_color = result_image[h//2, w//2]\n",
                "    mask = cv2.inRange(result_image, center_color, center_color)\n",
                "    kernel = np.ones((5, 5), np.uint8)\n",
                "    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel, iterations=2)\n",
                "    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
                "    if contours:\n",
                "        c = max(contours, key=cv2.contourArea)\n",
                "        mask_clean = np.zeros_like(mask)\n",
                "        cv2.drawContours(mask_clean, [c], -1, 255, -1)\n",
                "        mask = mask_clean\n",
                "    return mask\n",
                "\n",
                "print(\"‚úÖ K-Means function (Gamma=0.6, K=3)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 5: ENHANCED Feature Extraction"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def extract_features(image, mask):\n",
                "    \"\"\"ENHANCED feature extraction with more discriminative features\"\"\"\n",
                "    img = cv2.resize(image, (512, 512))\n",
                "    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
                "    if not contours: return None\n",
                "    \n",
                "    cnt = max(contours, key=cv2.contourArea)\n",
                "    area = cv2.contourArea(cnt)\n",
                "    perimeter = cv2.arcLength(cnt, True)\n",
                "    if area == 0 or perimeter == 0: return None\n",
                "    \n",
                "    # GEOMETRIC FEATURES\n",
                "    compactness = (perimeter ** 2) / area\n",
                "    approx = cv2.approxPolyDP(cnt, 0.01 * perimeter, True)\n",
                "    smoothness = len(approx) / perimeter\n",
                "    x, y, w, h = cv2.boundingRect(cnt)\n",
                "    aspect_ratio = float(w) / h if h > 0 else 0\n",
                "    rectangularity = area / (w * h) if (w * h) > 0 else 0\n",
                "    \n",
                "    # Additional shape features\n",
                "    hull = cv2.convexHull(cnt)\n",
                "    hull_area = cv2.contourArea(hull)\n",
                "    solidity = area / hull_area if hull_area > 0 else 0\n",
                "    \n",
                "    # Circularity\n",
                "    circularity = (4 * np.pi * area) / (perimeter ** 2) if perimeter > 0 else 0\n",
                "    \n",
                "    # COLOR FEATURES\n",
                "    mean_bgr = cv2.mean(img, mask=mask)\n",
                "    mean_red = mean_bgr[2]\n",
                "    mean_green = mean_bgr[1]\n",
                "    mean_blue = mean_bgr[0]\n",
                "    \n",
                "    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
                "    mean_hsv = cv2.mean(hsv, mask=mask)\n",
                "    mean_hue = mean_hsv[0]\n",
                "    mean_sat = mean_hsv[1]\n",
                "    mean_val = mean_hsv[2]\n",
                "    \n",
                "    # Color ratios (important for durian variety)\n",
                "    rg_ratio = mean_red / mean_green if mean_green > 0 else 0\n",
                "    rb_ratio = mean_red / mean_blue if mean_blue > 0 else 0\n",
                "    \n",
                "    return {\n",
                "        # Original features (for app.py compatibility)\n",
                "        'Compactness': compactness,\n",
                "        'Smoothness': smoothness,\n",
                "        'Aspect_Ratio': aspect_ratio,\n",
                "        'Rectangularity': rectangularity,\n",
                "        'Mean_Red': mean_red,\n",
                "        'Mean_Hue': mean_hue,\n",
                "        # Enhanced features\n",
                "        'Solidity': solidity,\n",
                "        'Circularity': circularity,\n",
                "        'Mean_Saturation': mean_sat,\n",
                "        'Mean_Value': mean_val,\n",
                "        'RG_Ratio': rg_ratio,\n",
                "        'RB_Ratio': rb_ratio\n",
                "    }\n",
                "\n",
                "print(\"‚úÖ Enhanced feature extraction (12 features)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 6: Data Augmentation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def augment_image(image):\n",
                "    \"\"\"Simple augmentation to increase training data\"\"\"\n",
                "    augmented = [image]\n",
                "    \n",
                "    # Horizontal flip\n",
                "    augmented.append(cv2.flip(image, 1))\n",
                "    \n",
                "    # Brightness variations\n",
                "    for factor in [0.8, 1.2]:\n",
                "        hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV).astype(np.float32)\n",
                "        hsv[:,:,2] = np.clip(hsv[:,:,2] * factor, 0, 255)\n",
                "        augmented.append(cv2.cvtColor(hsv.astype(np.uint8), cv2.COLOR_HSV2BGR))\n",
                "    \n",
                "    # Slight rotation\n",
                "    h, w = image.shape[:2]\n",
                "    for angle in [-10, 10]:\n",
                "        M = cv2.getRotationMatrix2D((w/2, h/2), angle, 1)\n",
                "        rotated = cv2.warpAffine(image, M, (w, h))\n",
                "        augmented.append(rotated)\n",
                "    \n",
                "    return augmented\n",
                "\n",
                "print(\"‚úÖ Augmentation function (6x data)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 7: Load and Process Dataset with Augmentation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Variety mapping - STRICT 3 classes only\n",
                "VARIETY_MAP = {\n",
                "    'musang': 'D197_MusangKing',\n",
                "    'd197': 'D197_MusangKing',\n",
                "    'king': 'D197_MusangKing',\n",
                "    'black': 'D200_BlackThorn',\n",
                "    'd200': 'D200_BlackThorn',\n",
                "    'thorn': 'D200_BlackThorn',\n",
                "    'udang': 'D175_UdangMerah',\n",
                "    'd175': 'D175_UdangMerah',\n",
                "    'merah': 'D175_UdangMerah',\n",
                "    'prawn': 'D175_UdangMerah'\n",
                "}\n",
                "\n",
                "def detect_variety(label_str):\n",
                "    \"\"\"Detect variety from any label string\"\"\"\n",
                "    label_lower = label_str.lower()\n",
                "    for key, variety in VARIETY_MAP.items():\n",
                "        if key in label_lower:\n",
                "            return variety\n",
                "    return None  # Unknown - skip\n",
                "\n",
                "def load_yolov5_dataset_with_augmentation(dataset_paths, use_augmentation=True):\n",
                "    all_data = []\n",
                "    variety_counts = {'D197_MusangKing': 0, 'D200_BlackThorn': 0, 'D175_UdangMerah': 0}\n",
                "    \n",
                "    for dataset_path in dataset_paths:\n",
                "        print(f\"\\nüìÇ Processing: {dataset_path}\")\n",
                "        \n",
                "        # Read class names from data.yaml\n",
                "        yaml_path = os.path.join(dataset_path, \"data.yaml\")\n",
                "        class_names = {}\n",
                "        if os.path.exists(yaml_path):\n",
                "            with open(yaml_path, 'r') as f:\n",
                "                data = yaml.safe_load(f)\n",
                "                names = data.get('names', {})\n",
                "                if isinstance(names, list):\n",
                "                    class_names = {i: n for i, n in enumerate(names)}\n",
                "                else:\n",
                "                    class_names = names\n",
                "            print(f\"  Classes: {class_names}\")\n",
                "        \n",
                "        for split in ['train', 'valid', 'test']:\n",
                "            images_dir = os.path.join(dataset_path, split, 'images')\n",
                "            labels_dir = os.path.join(dataset_path, split, 'labels')\n",
                "            \n",
                "            if not os.path.exists(images_dir): continue\n",
                "            \n",
                "            images = [f for f in os.listdir(images_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
                "            print(f\"  üìÅ {split}: {len(images)} images\")\n",
                "            \n",
                "            for img_file in tqdm(images, desc=f\"{split}\", leave=False):\n",
                "                try:\n",
                "                    img_path = os.path.join(images_dir, img_file)\n",
                "                    image = cv2.imread(img_path)\n",
                "                    if image is None: continue\n",
                "                    \n",
                "                    # Get class from label file\n",
                "                    label_file = os.path.splitext(img_file)[0] + '.txt'\n",
                "                    label_path = os.path.join(labels_dir, label_file)\n",
                "                    \n",
                "                    variety = None\n",
                "                    if os.path.exists(label_path):\n",
                "                        with open(label_path, 'r') as f:\n",
                "                            lines = f.readlines()\n",
                "                            if lines:\n",
                "                                class_id = int(lines[0].split()[0])\n",
                "                                class_name = class_names.get(class_id, '')\n",
                "                                variety = detect_variety(class_name)\n",
                "                    \n",
                "                    # Skip if not one of our 3 varieties\n",
                "                    if variety is None:\n",
                "                        continue\n",
                "                    \n",
                "                    # Apply augmentation or use original\n",
                "                    images_to_process = augment_image(image) if use_augmentation else [image]\n",
                "                    \n",
                "                    for aug_img in images_to_process:\n",
                "                        mask = get_mask_lab_method(aug_img)\n",
                "                        if cv2.countNonZero(mask) < (512 * 512 * 0.01): continue\n",
                "                        \n",
                "                        features = extract_features(aug_img, mask)\n",
                "                        if features:\n",
                "                            features['Variety'] = variety\n",
                "                            all_data.append(features)\n",
                "                            variety_counts[variety] += 1\n",
                "                            \n",
                "                except Exception as e:\n",
                "                    continue\n",
                "    \n",
                "    print(f\"\\nüìä Variety Distribution:\")\n",
                "    for v, c in variety_counts.items():\n",
                "        print(f\"   {v}: {c}\")\n",
                "    \n",
                "    return pd.DataFrame(all_data)\n",
                "\n",
                "# Find dataset paths\n",
                "DATASET_PATHS = [d for d in os.listdir('.') if 'durian' in d.lower() and os.path.isdir(d)]\n",
                "print(f\"Found datasets: {DATASET_PATHS}\")\n",
                "\n",
                "# Load with augmentation\n",
                "df = load_yolov5_dataset_with_augmentation(DATASET_PATHS, use_augmentation=True)\n",
                "print(f\"\\n‚úÖ Total samples: {len(df)}\")\n",
                "print(df['Variety'].value_counts())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 8: Train HIGH-CONFIDENCE Variety Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=\"*60)\n",
                "print(\"TRAINING HIGH-CONFIDENCE VARIETY MODEL\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "# Use original app.py compatible features\n",
                "VARIETY_FEATURES = ['Compactness', 'Smoothness', 'Aspect_Ratio', 'Rectangularity', 'Mean_Red']\n",
                "\n",
                "X = df[VARIETY_FEATURES]\n",
                "y = df['Variety']\n",
                "\n",
                "variety_encoder = LabelEncoder()\n",
                "y_encoded = variety_encoder.fit_transform(y)\n",
                "print(f\"\\nClasses: {variety_encoder.classes_}\")\n",
                "print(f\"Samples per class: {pd.Series(y_encoded).value_counts().to_dict()}\")\n",
                "\n",
                "# Feature Scaling\n",
                "scaler = StandardScaler()\n",
                "X_scaled = scaler.fit_transform(X)\n",
                "\n",
                "# Split\n",
                "X_train, X_test, y_train, y_test = train_test_split(\n",
                "    X_scaled, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
                ")\n",
                "\n",
                "# SMOTE\n",
                "smote = SMOTE(random_state=42)\n",
                "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
                "print(f\"After SMOTE: {pd.Series(y_train_smote).value_counts().to_dict()}\")\n",
                "\n",
                "# Optimized Ensemble\n",
                "print(\"\\nüîß Training Optimized Ensemble...\")\n",
                "\n",
                "# Individual models with optimized parameters\n",
                "svm = SVC(\n",
                "    probability=True, \n",
                "    kernel='rbf', \n",
                "    C=10.0,  # Higher regularization\n",
                "    gamma='scale',\n",
                "    random_state=42\n",
                ")\n",
                "\n",
                "rf = RandomForestClassifier(\n",
                "    n_estimators=200,  # More trees\n",
                "    max_depth=15,\n",
                "    min_samples_split=5,\n",
                "    class_weight='balanced',\n",
                "    random_state=42\n",
                ")\n",
                "\n",
                "xgb_clf = xgb.XGBClassifier(\n",
                "    n_estimators=200,\n",
                "    max_depth=6,\n",
                "    learning_rate=0.1,\n",
                "    subsample=0.8,\n",
                "    colsample_bytree=0.8,\n",
                "    random_state=42,\n",
                "    eval_metric='mlogloss'\n",
                ")\n",
                "\n",
                "gb = GradientBoostingClassifier(\n",
                "    n_estimators=100,\n",
                "    max_depth=5,\n",
                "    learning_rate=0.1,\n",
                "    random_state=42\n",
                ")\n",
                "\n",
                "variety_model = VotingClassifier(\n",
                "    estimators=[\n",
                "        ('svm', svm),\n",
                "        ('rf', rf),\n",
                "        ('xgb', xgb_clf),\n",
                "        ('gb', gb)\n",
                "    ],\n",
                "    voting='soft',\n",
                "    weights=[1, 1.5, 1.5, 1]  # Weight RF and XGB slightly higher\n",
                ")\n",
                "\n",
                "variety_model.fit(X_train_smote, y_train_smote)\n",
                "\n",
                "# Cross-validation\n",
                "print(\"\\nüìä Cross-Validation (5-fold):\")\n",
                "cv_scores = cross_val_score(variety_model, X_train_smote, y_train_smote, cv=5, scoring='accuracy')\n",
                "print(f\"   CV Scores: {cv_scores}\")\n",
                "print(f\"   Mean CV Accuracy: {cv_scores.mean()*100:.2f}% (+/- {cv_scores.std()*2*100:.2f}%)\")\n",
                "\n",
                "# Test set evaluation\n",
                "y_pred = variety_model.predict(X_test)\n",
                "y_pred_proba = variety_model.predict_proba(X_test)\n",
                "\n",
                "print(f\"\\nüéØ Test Set Accuracy: {accuracy_score(y_test, y_pred)*100:.2f}%\")\n",
                "print(\"\\nClassification Report:\")\n",
                "print(classification_report(y_test, y_pred, target_names=variety_encoder.classes_))\n",
                "\n",
                "# Confidence analysis\n",
                "confidences = np.max(y_pred_proba, axis=1)\n",
                "print(f\"\\nüìà Confidence Statistics:\")\n",
                "print(f\"   Mean: {confidences.mean()*100:.1f}%\")\n",
                "print(f\"   Min: {confidences.min()*100:.1f}%\")\n",
                "print(f\"   Max: {confidences.max()*100:.1f}%\")\n",
                "print(f\"   Samples > 80%: {(confidences > 0.8).sum()} / {len(confidences)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 9: Train Ripeness Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=\"*60)\n",
                "print(\"TRAINING RIPENESS MODEL\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "# For ripeness, we'll use a default since Roboflow doesn't have ripeness labels\n",
                "# We'll train on simulated data based on color features\n",
                "\n",
                "RIPENESS_FEATURES = ['Mean_Hue', 'Compactness', 'Smoothness']\n",
                "\n",
                "# Create synthetic ripeness labels based on color (simple heuristic)\n",
                "# Mature durian: lower hue (brownish)\n",
                "# Defective: higher saturation variance\n",
                "df['Ripeness'] = df.apply(\n",
                "    lambda row: 'defective' if row['Mean_Saturation'] < 50 or row['Compactness'] > 80 \n",
                "                else 'mature', axis=1\n",
                ")\n",
                "\n",
                "X_ripe = df[RIPENESS_FEATURES]\n",
                "y_ripe = df['Ripeness']\n",
                "\n",
                "ripeness_encoder = LabelEncoder()\n",
                "y_ripe_encoded = ripeness_encoder.fit_transform(y_ripe)\n",
                "print(f\"Classes: {ripeness_encoder.classes_}\")\n",
                "print(f\"Distribution: {pd.Series(y_ripe_encoded).value_counts().to_dict()}\")\n",
                "\n",
                "X_train_r, X_test_r, y_train_r, y_test_r = train_test_split(\n",
                "    X_ripe, y_ripe_encoded, test_size=0.2, random_state=42, stratify=y_ripe_encoded\n",
                ")\n",
                "\n",
                "smote_r = SMOTE(random_state=42)\n",
                "X_train_r_smote, y_train_r_smote = smote_r.fit_resample(X_train_r, y_train_r)\n",
                "\n",
                "ripeness_model = VotingClassifier(\n",
                "    estimators=[\n",
                "        ('svm', SVC(probability=True, random_state=42)),\n",
                "        ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
                "        ('xgb', xgb.XGBClassifier(n_estimators=100, random_state=42, eval_metric='mlogloss'))\n",
                "    ],\n",
                "    voting='soft'\n",
                ")\n",
                "\n",
                "ripeness_model.fit(X_train_r_smote, y_train_r_smote)\n",
                "\n",
                "y_pred_r = ripeness_model.predict(X_test_r)\n",
                "print(f\"\\nüéØ Ripeness Accuracy: {accuracy_score(y_test_r, y_pred_r)*100:.2f}%\")\n",
                "print(classification_report(y_test_r, y_pred_r, target_names=ripeness_encoder.classes_))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 10: Save Models"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "OUTPUT = \"TRAINING_MODEL\"\n",
                "os.makedirs(OUTPUT, exist_ok=True)\n",
                "\n",
                "joblib.dump(variety_model, f\"{OUTPUT}/variety_model.pkl\")\n",
                "joblib.dump(variety_encoder, f\"{OUTPUT}/variety_model_encoder.pkl\")\n",
                "joblib.dump(ripeness_model, f\"{OUTPUT}/ripeness_model.pkl\")\n",
                "joblib.dump(ripeness_encoder, f\"{OUTPUT}/ripeness_model_encoder.pkl\")\n",
                "df.to_csv(f\"{OUTPUT}/features.csv\", index=False)\n",
                "\n",
                "print(\"‚úÖ Models saved!\")\n",
                "!ls -la {OUTPUT}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!zip -r trained_models_v2.zip TRAINING_MODEL/\n",
                "\n",
                "from google.colab import files\n",
                "files.download('trained_models_v2.zip')\n",
                "\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"üéâ ENHANCED TRAINING COMPLETE!\")\n",
                "print(\"=\"*60)\n",
                "print(\"\\nImprovements in this version:\")\n",
                "print(\"‚úÖ Data augmentation (6x more samples)\")\n",
                "print(\"‚úÖ 4-model ensemble (SVM, RF, XGB, GB)\")\n",
                "print(\"‚úÖ Optimized hyperparameters\")\n",
                "print(\"‚úÖ Cross-validation verified\")\n",
                "print(\"\\nNext: Extract ZIP, copy .pkl to 'TRAINING MODEL', restart Flask\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}