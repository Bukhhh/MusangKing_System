{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üçà MusangKing Aligned Training Pipeline\n",
                "## Using K-Means Segmentation (Same as app.py)\n",
                "\n",
                "Downloads **BOTH** Roboflow datasets for maximum training data!"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 1: Install Dependencies"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install roboflow opencv-python-headless scikit-learn imbalanced-learn xgboost tqdm -q\n",
                "print(\"‚úÖ Dependencies installed!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 2: Download BOTH Datasets from Roboflow"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from roboflow import Roboflow\n",
                "\n",
                "API_KEY = \"EPa1uqQkD6BSITYjgrP3\"\n",
                "rf = Roboflow(api_key=API_KEY)\n",
                "\n",
                "# Download Dataset 1: Durian Own\n",
                "print(\"üì• Downloading Dataset 1: Durian Own...\")\n",
                "project1 = rf.workspace(\"durian-cf87w\").project(\"durian_own\")\n",
                "dataset1 = project1.version(1).download(\"folder\")\n",
                "print(\"‚úÖ Dataset 1 downloaded!\\n\")\n",
                "\n",
                "# Download Dataset 2: Durian Thesis\n",
                "print(\"üì• Downloading Dataset 2: Durian Thesis...\")\n",
                "project2 = rf.workspace(\"carl-bwzge\").project(\"durian-thesis\")\n",
                "dataset2 = project2.version(3).download(\"folder\")\n",
                "print(\"‚úÖ Dataset 2 downloaded!\")\n",
                "\n",
                "print(\"\\nüéâ Both datasets ready!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check downloaded folders\n",
                "!echo \"=== Downloaded Folders ===\"\n",
                "!ls -la\n",
                "!echo \"\"\n",
                "!echo \"=== Dataset 1 Structure ===\"\n",
                "!ls -la durian_own*/ 2>/dev/null || echo \"Not found with this name\"\n",
                "!echo \"\"\n",
                "!echo \"=== Dataset 2 Structure ===\"\n",
                "!ls -la durian-thesis*/ 2>/dev/null || echo \"Not found with this name\""
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 3: Set Dataset Paths\n",
                "\n",
                "**UPDATE THESE** based on the folder names shown above!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "\n",
                "# Auto-detect dataset paths\n",
                "DATASET_PATHS = []\n",
                "\n",
                "for folder in os.listdir('.'):\n",
                "    if 'durian' in folder.lower() and os.path.isdir(folder):\n",
                "        DATASET_PATHS.append(folder)\n",
                "        print(f\"‚úÖ Found: {folder}\")\n",
                "\n",
                "print(f\"\\nTotal datasets found: {len(DATASET_PATHS)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 4: Import Libraries"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import cv2\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "from tqdm import tqdm\n",
                "import joblib\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.preprocessing import LabelEncoder\n",
                "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
                "from sklearn.svm import SVC\n",
                "from sklearn.metrics import classification_report, accuracy_score\n",
                "from imblearn.over_sampling import SMOTE\n",
                "import xgboost as xgb\n",
                "\n",
                "print(\"‚úÖ Libraries imported!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 5: K-Means Segmentation (SAME AS APP.PY)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def get_mask_lab_method(image):\n",
                "    \"\"\"K-Means Segmentation - IDENTICAL TO APP.PY (Gamma=0.6, K=3)\"\"\"\n",
                "    img_resized = cv2.resize(image, (512, 512))\n",
                "    gamma = 0.6\n",
                "    table = np.array([((i / 255.0) ** (1.0/gamma)) * 255 for i in range(256)]).astype(\"uint8\")\n",
                "    img_gamma = cv2.LUT(img_resized, table)\n",
                "    img_lab = cv2.cvtColor(img_gamma, cv2.COLOR_BGR2Lab)\n",
                "    Z = img_lab.reshape((-1, 3)).astype(np.float32)\n",
                "    criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 10, 1.0)\n",
                "    _, label, center = cv2.kmeans(Z, 3, None, criteria, 10, cv2.KMEANS_RANDOM_CENTERS)\n",
                "    center = np.uint8(center)\n",
                "    result_image = center[label.flatten()].reshape((img_lab.shape))\n",
                "    h, w = result_image.shape[:2]\n",
                "    center_color = result_image[h//2, w//2]\n",
                "    mask = cv2.inRange(result_image, center_color, center_color)\n",
                "    kernel = np.ones((5, 5), np.uint8)\n",
                "    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel, iterations=2)\n",
                "    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
                "    if contours:\n",
                "        c = max(contours, key=cv2.contourArea)\n",
                "        mask_clean = np.zeros_like(mask)\n",
                "        cv2.drawContours(mask_clean, [c], -1, 255, -1)\n",
                "        mask = mask_clean\n",
                "    return mask\n",
                "\n",
                "print(\"‚úÖ K-Means function (Gamma=0.6, K=3)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 6: Feature Extraction (SAME AS APP.PY)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def extract_features(image, mask):\n",
                "    img = cv2.resize(image, (512, 512))\n",
                "    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
                "    if not contours: return None\n",
                "    cnt = max(contours, key=cv2.contourArea)\n",
                "    area = cv2.contourArea(cnt)\n",
                "    perimeter = cv2.arcLength(cnt, True)\n",
                "    if area == 0 or perimeter == 0: return None\n",
                "    compactness = (perimeter ** 2) / area\n",
                "    approx = cv2.approxPolyDP(cnt, 0.01 * perimeter, True)\n",
                "    smoothness = len(approx) / perimeter\n",
                "    x, y, w, h = cv2.boundingRect(cnt)\n",
                "    aspect_ratio = float(w) / h if h > 0 else 0\n",
                "    rectangularity = area / (w * h) if (w * h) > 0 else 0\n",
                "    mean_red = cv2.mean(img, mask=mask)[2]\n",
                "    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
                "    mean_hue = cv2.mean(hsv, mask=mask)[0]\n",
                "    return {'Compactness': compactness, 'Smoothness': smoothness, 'Aspect_Ratio': aspect_ratio,\n",
                "            'Rectangularity': rectangularity, 'Mean_Red': mean_red, 'Mean_Hue': mean_hue}\n",
                "\n",
                "print(\"‚úÖ Feature extraction function\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 7: Process ALL Datasets"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def load_all_datasets(dataset_paths):\n",
                "    all_data = []\n",
                "    \n",
                "    for dataset_path in dataset_paths:\n",
                "        print(f\"\\nüìÇ Processing: {dataset_path}\")\n",
                "        \n",
                "        # Walk through all subdirectories\n",
                "        for root, dirs, files in os.walk(dataset_path):\n",
                "            images = [f for f in files if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
                "            if not images:\n",
                "                continue\n",
                "            \n",
                "            # Try to extract variety and ripeness from path\n",
                "            path_parts = root.replace(dataset_path, '').strip('/\\\\').split(os.sep)\n",
                "            \n",
                "            # Default labels (will be overwritten if detected)\n",
                "            variety = 'Unknown'\n",
                "            ripeness = 'Unknown'\n",
                "            \n",
                "            for part in path_parts:\n",
                "                part_lower = part.lower()\n",
                "                # Detect variety\n",
                "                if 'musang' in part_lower or 'd197' in part_lower:\n",
                "                    variety = 'D197_MusangKing'\n",
                "                elif 'black' in part_lower or 'd200' in part_lower:\n",
                "                    variety = 'D200_BlackThorn'\n",
                "                elif 'udang' in part_lower or 'd175' in part_lower or 'merah' in part_lower:\n",
                "                    variety = 'D175_UdangMerah'\n",
                "                \n",
                "                # Detect ripeness\n",
                "                if 'mature' in part_lower and 'immature' not in part_lower:\n",
                "                    ripeness = 'mature'\n",
                "                elif 'immature' in part_lower:\n",
                "                    ripeness = 'immature'\n",
                "                elif 'defective' in part_lower or 'defect' in part_lower:\n",
                "                    ripeness = 'defective'\n",
                "            \n",
                "            if variety == 'Unknown':\n",
                "                continue  # Skip if can't determine variety\n",
                "            \n",
                "            print(f\"  üìÅ {variety}/{ripeness}: {len(images)} images\")\n",
                "            \n",
                "            for img_file in tqdm(images, desc=f\"{variety}/{ripeness}\", leave=False):\n",
                "                try:\n",
                "                    img_path = os.path.join(root, img_file)\n",
                "                    image = cv2.imread(img_path)\n",
                "                    if image is None: continue\n",
                "                    \n",
                "                    mask = get_mask_lab_method(image)\n",
                "                    if cv2.countNonZero(mask) < (512 * 512 * 0.01): continue\n",
                "                    \n",
                "                    features = extract_features(image, mask)\n",
                "                    if features:\n",
                "                        features['Variety'] = variety\n",
                "                        features['Ripeness'] = ripeness\n",
                "                        all_data.append(features)\n",
                "                except:\n",
                "                    continue\n",
                "    \n",
                "    return pd.DataFrame(all_data)\n",
                "\n",
                "print(\"Loading from:\", DATASET_PATHS)\n",
                "df = load_all_datasets(DATASET_PATHS)\n",
                "print(f\"\\n‚úÖ Total processed: {len(df)} images!\")\n",
                "print(\"\\nSummary:\")\n",
                "print(df[['Variety', 'Ripeness']].value_counts())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 8: Train Variety Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"TRAINING VARIETY MODEL\")\n",
                "\n",
                "X = df[['Compactness', 'Smoothness', 'Aspect_Ratio', 'Rectangularity', 'Mean_Red']]\n",
                "y = df['Variety']\n",
                "\n",
                "variety_encoder = LabelEncoder()\n",
                "y_encoded = variety_encoder.fit_transform(y)\n",
                "print(f\"Classes: {variety_encoder.classes_}\")\n",
                "\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded)\n",
                "\n",
                "smote = SMOTE(random_state=42)\n",
                "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
                "\n",
                "variety_model = VotingClassifier(\n",
                "    estimators=[('svm', SVC(probability=True, random_state=42)),\n",
                "                ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
                "                ('xgb', xgb.XGBClassifier(n_estimators=100, random_state=42, eval_metric='mlogloss'))],\n",
                "    voting='soft')\n",
                "variety_model.fit(X_train_smote, y_train_smote)\n",
                "\n",
                "print(f\"\\nüéØ Accuracy: {accuracy_score(y_test, variety_model.predict(X_test))*100:.2f}%\")\n",
                "print(classification_report(y_test, variety_model.predict(X_test), target_names=variety_encoder.classes_))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 9: Train Ripeness Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Filter out 'Unknown' ripeness\n",
                "df_ripe = df[df['Ripeness'] != 'Unknown']\n",
                "print(f\"Training ripeness on {len(df_ripe)} samples\")\n",
                "\n",
                "if len(df_ripe) > 0:\n",
                "    X = df_ripe[['Mean_Hue', 'Compactness', 'Smoothness']]\n",
                "    y = df_ripe['Ripeness']\n",
                "\n",
                "    ripeness_encoder = LabelEncoder()\n",
                "    y_encoded = ripeness_encoder.fit_transform(y)\n",
                "    print(f\"Classes: {ripeness_encoder.classes_}\")\n",
                "\n",
                "    X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded)\n",
                "\n",
                "    smote = SMOTE(random_state=42)\n",
                "    X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
                "\n",
                "    ripeness_model = VotingClassifier(\n",
                "        estimators=[('svm', SVC(probability=True, random_state=42)),\n",
                "                    ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
                "                    ('xgb', xgb.XGBClassifier(n_estimators=100, random_state=42, eval_metric='mlogloss'))],\n",
                "        voting='soft')\n",
                "    ripeness_model.fit(X_train_smote, y_train_smote)\n",
                "\n",
                "    print(f\"\\nüéØ Accuracy: {accuracy_score(y_test, ripeness_model.predict(X_test))*100:.2f}%\")\n",
                "    print(classification_report(y_test, ripeness_model.predict(X_test), target_names=ripeness_encoder.classes_))\n",
                "else:\n",
                "    print(\"‚ö†Ô∏è No ripeness labels found - skipping ripeness model\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 10: Save & Download"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "OUTPUT = \"TRAINING_MODEL\"\n",
                "os.makedirs(OUTPUT, exist_ok=True)\n",
                "\n",
                "joblib.dump(variety_model, f\"{OUTPUT}/variety_model.pkl\")\n",
                "joblib.dump(variety_encoder, f\"{OUTPUT}/variety_model_encoder.pkl\")\n",
                "if 'ripeness_model' in dir():\n",
                "    joblib.dump(ripeness_model, f\"{OUTPUT}/ripeness_model.pkl\")\n",
                "    joblib.dump(ripeness_encoder, f\"{OUTPUT}/ripeness_model_encoder.pkl\")\n",
                "df.to_csv(f\"{OUTPUT}/features.csv\", index=False)\n",
                "\n",
                "print(\"‚úÖ Models saved!\")\n",
                "!ls -la {OUTPUT}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!zip -r trained_models.zip TRAINING_MODEL/\n",
                "\n",
                "from google.colab import files\n",
                "files.download('trained_models.zip')\n",
                "\n",
                "print(\"\\nüéâ DONE!\")\n",
                "print(\"1. Extract trained_models.zip\")\n",
                "print(\"2. Copy .pkl files to 'TRAINING MODEL' folder\")\n",
                "print(\"3. Restart Flask app\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}